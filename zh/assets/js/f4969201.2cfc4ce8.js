"use strict";(self.webpackChunkwww=self.webpackChunkwww||[]).push([[1994],{3905:(e,t,n)=>{n.d(t,{Zo:()=>d,kt:()=>p});var a=n(67294);function i(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){i(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,a,i=function(e,t){if(null==e)return{};var n,a,i={},r=Object.keys(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||(i[n]=e[n]);return i}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(i[n]=e[n])}return i}var s=a.createContext({}),m=function(e){var t=a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},d=function(e){var t=m(e.components);return a.createElement(s.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},c=a.forwardRef((function(e,t){var n=e.components,i=e.mdxType,r=e.originalType,s=e.parentName,d=l(e,["components","mdxType","originalType","parentName"]),c=m(n),p=i,h=c["".concat(s,".").concat(p)]||c[p]||u[p]||r;return n?a.createElement(h,o(o({ref:t},d),{},{components:n})):a.createElement(h,o({ref:t},d))}));function p(e,t){var n=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var r=n.length,o=new Array(r);o[0]=c;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l.mdxType="string"==typeof e?e:i,o[1]=l;for(var m=2;m<r;m++)o[m]=n[m];return a.createElement.apply(null,o)}return a.createElement.apply(null,n)}c.displayName="MDXCreateElement"},4064:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>o,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>m});var a=n(87462),i=(n(67294),n(3905));const r={title:"DORA - Median Time to Restore Service",description:"DORA - Median Time to Restore Service\n",sidebar_position:20},o=void 0,l={unversionedId:"Metrics/MTTR",id:"Metrics/MTTR",title:"DORA - Median Time to Restore Service",description:"DORA - Median Time to Restore Service\n",source:"@site/docs/Metrics/MTTR.md",sourceDirName:"Metrics",slug:"/Metrics/MTTR",permalink:"/zh/docs/Metrics/MTTR",draft:!1,editUrl:"https://github.com/apache/incubator-devlake-website/edit/main/docs/Metrics/MTTR.md",tags:[],version:"current",sidebarPosition:20,frontMatter:{title:"DORA - Median Time to Restore Service",description:"DORA - Median Time to Restore Service\n",sidebar_position:20},sidebar:"docsSidebar",previous:{title:"DORA - Lead Time for Changes",permalink:"/zh/docs/Metrics/LeadTimeForChanges"},next:{title:"DORA - Change Failure Rate",permalink:"/zh/docs/Metrics/CFR"}},s={},m=[{value:"What is this metric?",id:"what-is-this-metric",level:2},{value:"Why is it important?",id:"why-is-it-important",level:2},{value:"Which dashboard(s) does it exist in",id:"which-dashboards-does-it-exist-in",level:2},{value:"How is it calculated?",id:"how-is-it-calculated",level:2},{value:"How to improve?",id:"how-to-improve",level:2}],d={toc:m};function u(e){let{components:t,...r}=e;return(0,i.kt)("wrapper",(0,a.Z)({},d,r,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h2",{id:"what-is-this-metric"},"What is this metric?"),(0,i.kt)("p",null,"The time to restore service after service incidents, rollbacks, or any type of production failure happened."),(0,i.kt)("h2",{id:"why-is-it-important"},"Why is it important?"),(0,i.kt)("p",null,"This metric is essential to measure the disaster control capability of your team and the robustness of the software."),(0,i.kt)("h2",{id:"which-dashboards-does-it-exist-in"},"Which dashboard(s) does it exist in"),(0,i.kt)("p",null,"DORA dashboard. See ",(0,i.kt)("a",{parentName:"p",href:"https://grafana-lake.demo.devlake.io/grafana/d/qNo8_0M4z/dora?orgId=1"},"live demo"),"."),(0,i.kt)("h2",{id:"how-is-it-calculated"},"How is it calculated?"),(0,i.kt)("p",null,"MTTR = Total ",(0,i.kt)("a",{parentName:"p",href:"/zh/docs/Metrics/IncidentAge"},"incident age")," (in hours)/number of incidents."),(0,i.kt)("p",null,"If you have three incidents that happened in the given data range, one lasting 1 hour, one lasting 2 hours and one lasting 3 hours. Your MTTR will be: (1 + 2 + 3) / 3 = 2 hours."),(0,i.kt)("p",null,"Below are the benchmarks for different development teams from Google's report. However, it's difficult to tell which group a team falls into when the team's median time to restore service is ",(0,i.kt)("inlineCode",{parentName:"p"},"between one week and six months"),". Therefore, DevLake provides its own benchmarks to address this problem:"),(0,i.kt)("table",null,(0,i.kt)("thead",{parentName:"table"},(0,i.kt)("tr",{parentName:"thead"},(0,i.kt)("th",{parentName:"tr",align:null},"Groups"),(0,i.kt)("th",{parentName:"tr",align:null},"Benchmarks"),(0,i.kt)("th",{parentName:"tr",align:null},"DevLake Benchmarks"))),(0,i.kt)("tbody",{parentName:"table"},(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"Elite performers"),(0,i.kt)("td",{parentName:"tr",align:null},"Less than one hour"),(0,i.kt)("td",{parentName:"tr",align:null},"Less than one hour")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"High performers"),(0,i.kt)("td",{parentName:"tr",align:null},"Less one day"),(0,i.kt)("td",{parentName:"tr",align:null},"Less than one day")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"Medium performers"),(0,i.kt)("td",{parentName:"tr",align:null},"Between one day and one week"),(0,i.kt)("td",{parentName:"tr",align:null},"Between one day and one week")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"Low performers"),(0,i.kt)("td",{parentName:"tr",align:null},"More than six months"),(0,i.kt)("td",{parentName:"tr",align:null},"More than one week")))),(0,i.kt)("p",null,(0,i.kt)("i",null,"Source: 2021 Accelerate State of DevOps, Google")),(0,i.kt)("b",null,"Data Sources Required"),(0,i.kt)("p",null,"This metric relies on:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"Deployments")," collected in one of the following ways:",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Open APIs of Jenkins, GitLab, GitHub, etc."),(0,i.kt)("li",{parentName:"ul"},"Webhook for general CI tools."),(0,i.kt)("li",{parentName:"ul"},"Releases and PR/MRs from GitHub, GitLab APIs, etc."))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"Incidents")," collected in one of the following ways:",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Issue tracking tools such as Jira, TAPD, GitHub, etc."),(0,i.kt)("li",{parentName:"ul"},"Bug or Service Monitoring tools such as PagerDuty, Sentry, etc."),(0,i.kt)("li",{parentName:"ul"},"CI pipelines that marked the 'failed' deployments.")))),(0,i.kt)("b",null,"Transformation Rules Required"),(0,i.kt)("p",null,"This metric relies on:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Deployment configuration in Jenkins, GitLab or GitHub transformation rules to let DevLake know what CI builds/jobs can be regarded as ",(0,i.kt)("inlineCode",{parentName:"li"},"Deployments"),"."),(0,i.kt)("li",{parentName:"ul"},"Incident configuration in Jira, GitHub or TAPD transformation rules to let DevLake know what CI builds/jobs can be regarded as ",(0,i.kt)("inlineCode",{parentName:"li"},"Incidents"),".")),(0,i.kt)("b",null,"SQL Queries"),(0,i.kt)("p",null,"If you want to measure the monthly trend of median time to restore service as the picture shown below, run the following SQL in Grafana."),(0,i.kt)("p",null,(0,i.kt)("img",{src:n(40518).Z,width:"1354",height:"602"})),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"with _incidents as (\n-- get the incident count each month\n    SELECT\n        date_format(created_date,'%y/%m') as month,\n        cast(lead_time_minutes as signed) as lead_time_minutes\n    FROM\n        issues\n    WHERE\n        type = 'INCIDENT'\n),\n\n_find_median_mttr_each_month as (\n    SELECT \n        x.*\n    from _incidents x join _incidents y on x.month = y.month\n    WHERE x.lead_time_minutes is not null and y.lead_time_minutes is not null\n    GROUP BY x.month, x.lead_time_minutes\n    HAVING SUM(SIGN(1-SIGN(y.lead_time_minutes-x.lead_time_minutes)))/COUNT(*) > 0.5\n),\n\n_find_mttr_rank_each_month as (\n    SELECT\n        *,\n        rank() over(PARTITION BY month ORDER BY lead_time_minutes) as _rank \n    FROM\n        _find_median_mttr_each_month\n),\n\n_mttr as (\n    SELECT\n        month,\n        lead_time_minutes as med_time_to_resolve\n    from _find_mttr_rank_each_month\n    WHERE _rank = 1\n),\n\n_calendar_months as(\n-- deal with the month with no incidents\n    SELECT date_format(CAST((SYSDATE()-INTERVAL (month_index) MONTH) AS date), '%y/%m') as month\n    FROM ( SELECT 0 month_index\n            UNION ALL SELECT   1  UNION ALL SELECT   2 UNION ALL SELECT   3\n            UNION ALL SELECT   4  UNION ALL SELECT   5 UNION ALL SELECT   6\n            UNION ALL SELECT   7  UNION ALL SELECT   8 UNION ALL SELECT   9\n            UNION ALL SELECT   10 UNION ALL SELECT  11\n        ) month_index\n    WHERE (SYSDATE()-INTERVAL (month_index) MONTH) > SYSDATE()-INTERVAL 6 MONTH \n)\n\nSELECT \n    cm.month,\n    case \n        when m.med_time_to_resolve is null then 0 \n        else m.med_time_to_resolve/60 end as med_time_to_resolve_in_hour\nFROM \n    _calendar_months cm\n    left join _mttr m on cm.month = m.month\nORDER BY 1\n")),(0,i.kt)("p",null,"If you want to measure in which category your team falls into as the picture shown below, run the following SQL in Grafana."),(0,i.kt)("p",null,(0,i.kt)("img",{src:n(65213).Z,width:"672",height:"378"})),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},'with _incidents as (\n-- get the incidents created within the selected time period in the top-right corner\n    SELECT\n        cast(lead_time_minutes as signed) as lead_time_minutes\n    FROM\n        issues\n    WHERE\n        type = \'INCIDENT\'\n        and $__timeFilter(created_date)\n),\n\n_median_mttr as (\n    SELECT \n        x.lead_time_minutes as med_time_to_resolve\n    from _incidents x, _incidents y\n    WHERE x.lead_time_minutes is not null and y.lead_time_minutes is not null\n    GROUP BY x.lead_time_minutes\n    HAVING SUM(SIGN(1-SIGN(y.lead_time_minutes-x.lead_time_minutes)))/COUNT(*) > 0.5\n    LIMIT 1\n)\n\nSELECT \n    case\n        WHEN med_time_to_resolve < 60  then "Less than one hour"\n    WHEN med_time_to_resolve < 24 * 60 then "Less than one Day"\n    WHEN med_time_to_resolve < 7 * 24 * 60  then "Between one day and one week"\n    ELSE "More than one week"\n    END as med_time_to_resolve\nFROM \n    _median_mttr\n')),(0,i.kt)("h2",{id:"how-to-improve"},"How to improve?"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Use automated tools to quickly report failure"),(0,i.kt)("li",{parentName:"ul"},"Prioritize recovery when a failure happens"),(0,i.kt)("li",{parentName:"ul"},"Establish a go-to action plan to respond to failures immediately"),(0,i.kt)("li",{parentName:"ul"},"Reduce the deployment time for failure-fixing")))}u.isMDXComponent=!0},40518:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/mttr-monthly-71f037b867c1f94523aa18ca7cd2e497.jpeg"},65213:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/mttr-text-d8b5e6799ddda9c692bd6a8fbdf05b0f.jpeg"}}]);